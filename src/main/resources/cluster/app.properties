# Properties file for cluster.
storage.url = hdfs://sydney.informatik.privat:8020
storage.dir = /user/teamproject2016/parquet/

hadoop.user = teamproject2016
hadoop.homefolder = hdfs://sydney.informatik.privat:8020/user/teamproject2016/
hadoop.numpartitions = 18

spark.app.name = Spark RDF Analyzer
spark.master = spark://sydney.informatik.privat:7077

spark.executor.memory = 2g
spark.sql.parquet.binaryAsString = true
spark.core.connection.ack.wait.timeout = 200
spark.core.connection.auth.wait.timeout = 200
spark.akka.timeout = 200
spark.storage.blockManagerSlaveTimeoutMs = 200000
spark.shuffle.io.connectionTimeout = 200
spark.sql.parquet.filterPushdown = true
spark.rdd.compress = true
spark.default.parallelism = 32
spark.sql.inMemoryColumnarStorage.compressed = true
spark.sql.shuffle.partitions = 32

# spark.eventLog.enabled = true;
# spark.eventLog.dir = hdfs://isydney.informatik.uni-freiburg.de:8020/user/teamproject2016/logs/;
# spark.sql.codegen = true;
